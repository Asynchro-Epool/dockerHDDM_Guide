tau_hat <- max(tau_hat, 1/log10(ess))
ess <- ess / tau_hat
ess
}
n_eff_vec <- apply(x, 3, ess_rfun)
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
autocovariance <- function(y) {
# Compute autocovariance estimates for every lag for the specified
# input sequence using a fast Fourier transform approach.
N <- length(y)
M <- fft_next_good_size(N)
Mt2 <- 2 * M
yc <- y - mean(y)
yc <- c(yc, rep.int(0, Mt2-N))
transform <- stats::fft(yc)
ac <- stats::fft(Conj(transform) * transform, inverse = TRUE)
# use "biased" estimate as recommended by Geyer (1992)
ac <- Re(ac)[1:N] / (N^2 * 2)
ac
}
fft_next_good_size <- function(N) {
# Find the optimal next size for the FFT so that
# a minimum number of zeros are padded.
if (N <= 2)
return(2)
while (TRUE) {
m = N
while ((m %% 2) == 0) m = m / 2
while ((m %% 3) == 0) m = m / 3
while ((m %% 5) == 0) m = m / 5
if (m <= 1)
return(N)
N = N + 1
}
}
ess_rfun <- function(sims) {
# Compute the effective sample size for samples of several chains
# for one parameter; see the C++ code of function
# effective_sample_size in chains.cpp
#
# Args:
#   sims: a 2-d array _without_ warmup samples (# iter * # chains)
#
if (is.vector(sims)) dim(sims) <- c(length(sims), 1)
chains <- ncol(sims)
n_samples <- nrow(sims)
acov <- lapply(1:chains, FUN = function(i) autocovariance(sims[,i]))
acov <- do.call(cbind, acov)
chain_mean <- colMeans(sims)
mean_var <- mean(acov[1,]) * n_samples / (n_samples - 1)
var_plus <- mean_var * (n_samples - 1) / n_samples
if (chains > 1)
var_plus <- var_plus + var(chain_mean)
# Geyer's initial positive sequence
rho_hat_t <- rep.int(0, n_samples)
t <- 0
rho_hat_even <- 1
rho_hat_t[t + 1] <- rho_hat_even
rho_hat_odd <- 1 - (mean_var - mean(acov[t + 2, ])) / var_plus
rho_hat_t[t + 2] <- rho_hat_odd
while (t < nrow(acov) - 5 && !is.nan(rho_hat_even + rho_hat_odd) &&
(rho_hat_even + rho_hat_odd > 0)) {
t <- t + 2
rho_hat_even = 1 - (mean_var - mean(acov[t + 1, ])) / var_plus
rho_hat_odd = 1 - (mean_var - mean(acov[t + 2, ])) / var_plus
if ((rho_hat_even + rho_hat_odd) >= 0) {
rho_hat_t[t + 1] <- rho_hat_even
rho_hat_t[t + 2] <- rho_hat_odd
}
}
max_t <- t
# this is used in the improved estimate
if (rho_hat_even>0)
rho_hat_t[max_t + 1] <- rho_hat_even
# Geyer's initial monotone sequence
t <- 0
while (t <= max_t - 4) {
t <- t + 2
if (rho_hat_t[t + 1] + rho_hat_t[t + 2] >
rho_hat_t[t - 1] + rho_hat_t[t]) {
rho_hat_t[t + 1] = (rho_hat_t[t - 1] + rho_hat_t[t]) / 2;
rho_hat_t[t + 2] = rho_hat_t[t + 1];
}
}
ess <- chains * n_samples
# Geyer's truncated estimate
# tau_hat <- -1 + 2 * sum(rho_hat_t[1:max_t])
# Improved estimate reduces variance in antithetic case
tau_hat <- -1 + 2 * sum(rho_hat_t[1:max_t]) + rho_hat_t[max_t+1]
# Safety check for negative values and with max ess equal to ess*log10(ess)
tau_hat <- max(tau_hat, 1/log10(ess))
ess <- ess / tau_hat
ess
}
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
mean(r_eff)
data_2 <- data[2, , drop = FALSE]
tmp2 <- llfun_logistic(data_i = data_2, draws = draws)
tmp_ <- as.vector(tmp2)
dim(tmp_) <- c(length(tmp_), 1)
class(tmp_) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
# import the data from python
tmp_py <- read.csv("tmp.csv")
View(tmp_py)
# import the data from python
tmp_py <- read.csv("tmp.csv")
# import the data from python
tmp_py <- read.csv("tmp.csv")
View(tmp_py)
# import the data from python
tmp_py <- t(read.csv("tmp.csv"))
# import the data from python
tmp_py <- as.vector(t(read.csv("tmp.csv")))
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
dim(tmp_py) <- c(length(tmp_py), 1)
class(tmp_py) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
fit_1
chain_id <- rep(1:4, each=1000)
data <- stan_df_1
draws <- parameter_draws_1
x <- llfun_logistic
N <- dim(data)[1]
n_eff_list <-
lapply(
X = seq_len(N),
FUN = function(i) {
val_i <- llfun_logistic(data_i = data[i, , drop = FALSE], draws = draws)
#relative_eff.default(as.vector(val_i), chain_id = chain_id, cores = 1)
}
)
data_i <- data[1, , drop = FALSE]
tmp <- llfun_logistic(data_i = data_i, draws = draws)
plot(tmp)
hist(tmp)
data_i <- data[101, , drop = FALSE]
tmp <- llfun_logistic(data_i = data_i, draws = draws)
hist(tmp)
data_2 <- data[99, , drop = FALSE]
tmp2 <- llfun_logistic(data_i = data_2, draws = draws)
mean(tmp2)
data_2 <- data[100, , drop = FALSE]
tmp2 <- llfun_logistic(data_i = data_2, draws = draws)
mean(tmp2)
tmp_ <- as.vector(tmp2)
dim(tmp_) <- c(length(tmp_), 1)
class(tmp_) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_, chain_id) # from matrix to 1000 * 4 * 3020 array
llmatrix_to_array <- function(x, chain_id) {
stopifnot(is.matrix(x), all(chain_id == as.integer(chain_id)))
lldim <- dim(x)
n_chain <- length(unique(chain_id))
chain_id <- as.integer(chain_id)
chain_counts <- as.numeric(table(chain_id))
if (length(chain_id) != lldim[1]) {
stop("Number of rows in matrix not equal to length(chain_id).",
call. = FALSE)
} else if (any(chain_counts != chain_counts[1])) {
stop("Not all chains have same number of iterations.",
call. = FALSE)
} else if (max(chain_id) != n_chain) {
stop("max(chain_id) not equal to the number of chains.",
call. = FALSE)
}
n_iter <- lldim[1] / n_chain
n_obs <- lldim[2]
a <- array(data = NA, dim = c(n_iter, n_chain, n_obs))
for (c in seq_len(n_chain)) {
a[, c, ] <- x[chain_id == c, , drop = FALSE]
}
return(a)
}
ess_rfun <- function(sims) {
# Compute the effective sample size for samples of several chains
# for one parameter; see the C++ code of function
# effective_sample_size in chains.cpp
#
# Args:
#   sims: a 2-d array _without_ warmup samples (# iter * # chains)
#
if (is.vector(sims)) dim(sims) <- c(length(sims), 1)
chains <- ncol(sims)
n_samples <- nrow(sims)
acov <- lapply(1:chains, FUN = function(i) autocovariance(sims[,i]))
acov <- do.call(cbind, acov)
chain_mean <- colMeans(sims)
mean_var <- mean(acov[1,]) * n_samples / (n_samples - 1)
var_plus <- mean_var * (n_samples - 1) / n_samples
if (chains > 1)
var_plus <- var_plus + var(chain_mean)
# Geyer's initial positive sequence
rho_hat_t <- rep.int(0, n_samples)
t <- 0
rho_hat_even <- 1
rho_hat_t[t + 1] <- rho_hat_even
rho_hat_odd <- 1 - (mean_var - mean(acov[t + 2, ])) / var_plus
rho_hat_t[t + 2] <- rho_hat_odd
while (t < nrow(acov) - 5 && !is.nan(rho_hat_even + rho_hat_odd) &&
(rho_hat_even + rho_hat_odd > 0)) {
t <- t + 2
rho_hat_even = 1 - (mean_var - mean(acov[t + 1, ])) / var_plus
rho_hat_odd = 1 - (mean_var - mean(acov[t + 2, ])) / var_plus
if ((rho_hat_even + rho_hat_odd) >= 0) {
rho_hat_t[t + 1] <- rho_hat_even
rho_hat_t[t + 2] <- rho_hat_odd
}
}
max_t <- t
# this is used in the improved estimate
if (rho_hat_even>0)
rho_hat_t[max_t + 1] <- rho_hat_even
# Geyer's initial monotone sequence
t <- 0
while (t <= max_t - 4) {
t <- t + 2
if (rho_hat_t[t + 1] + rho_hat_t[t + 2] >
rho_hat_t[t - 1] + rho_hat_t[t]) {
rho_hat_t[t + 1] = (rho_hat_t[t - 1] + rho_hat_t[t]) / 2;
rho_hat_t[t + 2] = rho_hat_t[t + 1];
}
}
ess <- chains * n_samples
# Geyer's truncated estimate
# tau_hat <- -1 + 2 * sum(rho_hat_t[1:max_t])
# Improved estimate reduces variance in antithetic case
tau_hat <- -1 + 2 * sum(rho_hat_t[1:max_t]) + rho_hat_t[max_t+1]
# Safety check for negative values and with max ess equal to ess*log10(ess)
tau_hat <- max(tau_hat, 1/log10(ess))
ess <- ess / tau_hat
ess
}
autocovariance <- function(y) {
# Compute autocovariance estimates for every lag for the specified
# input sequence using a fast Fourier transform approach.
N <- length(y)
M <- fft_next_good_size(N)
Mt2 <- 2 * M
yc <- y - mean(y)
yc <- c(yc, rep.int(0, Mt2-N))
transform <- stats::fft(yc)
ac <- stats::fft(Conj(transform) * transform, inverse = TRUE)
# use "biased" estimate as recommended by Geyer (1992)
ac <- Re(ac)[1:N] / (N^2 * 2)
ac
}
fft_next_good_size <- function(N) {
# Find the optimal next size for the FFT so that
# a minimum number of zeros are padded.
if (N <= 2)
return(2)
while (TRUE) {
m = N
while ((m %% 2) == 0) m = m / 2
while ((m %% 3) == 0) m = m / 3
while ((m %% 5) == 0) m = m / 5
if (m <= 1)
return(N)
N = N + 1
}
}
data_2 <- data[100, , drop = FALSE]
tmp2 <- llfun_logistic(data_i = data_2, draws = draws)
mean(tmp2)
tmp_ <- as.vector(tmp2)
dim(tmp_) <- c(length(tmp_), 1)
class(tmp_) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
View(n_eff_list)
n_eff_list <-
lapply(
X = seq_len(N),
FUN = function(i) {
val_i <- llfun_logistic(data_i = data[i, , drop = FALSE], draws = draws)
relative_eff.default(as.vector(val_i), chain_id = chain_id, cores = 1)
}
)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
r_eff[100]
n_eff_vec/S
# import the data from python and check the results
tmp_py <- as.vector(t(read.csv("tmp.csv")))
dim(tmp_py) <- c(length(tmp_py), 1)
class(tmp_py) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
# import the data from python and check the results
tmp_py <- as.vector(t(read.csv("tmp_100.csv")))
dim(tmp_py) <- c(length(tmp_py), 1)
class(tmp_py) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
r_eff[1]
mean(r_eff)
hist(n_eff_list[[1]])
mean(n_eff_list[[1]])
tmp_ <- as.vector(n_eff_list)
dim(tmp_) <- c(length(tmp_), 1)
class(tmp_) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_, chain_id) # from matrix to 1000 * 4 * 3020 array
View(tmp_)
tmp_ <- as.vector(n_eff_list[[1]])
dim(tmp_) <- c(length(tmp_), 1)
class(tmp_) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
r_eff[1]
tmp_4 <- tmp_0[:, :, 1]
tmp_4 <- tmp_0[, , 1]
View(tmp_4)
tmp_4 <- tmp_0[, , 1] %>%
write_csv(., "tmp_1_2.csv")
tmp_4 <- tmp_0[, , 1] %>%
write_csv("tmp_1_2.csv")
write_csv(tmp_4, "tmp_1_2.csv")
write_csv(as.data.frame(tmp_4), "tmp_1_2.csv")
?write_csv
tmp_ <- as.vector(n_eff_list[[1]])
dim(tmp_) <- c(length(tmp_), 1)
class(tmp_) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
# import the data from python and check the results
tmp_py <- as.vector(t(read.csv("tmp_100.csv")))
tmp_py <- exp(tmp_py)
dim(tmp_py) <- c(length(tmp_py), 1)
class(tmp_py) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
# import the data from python and check the results
tmp_py <- as.vector(t(read.csv("tmp_100.csv")))
tmp_py <- exp(tmp_py)
dim(tmp_py) <- c(length(tmp_py), 1)
class(tmp_py) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
r_eff[100]
# import the data from python and check the results
tmp_py <- as.vector(t(read.csv("tmp_100.csv")))
dim(tmp_py) <- c(length(tmp_py), 1)
class(tmp_py) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
# import the data from python and check the results
tmp_py <- as.vector(t(read.csv("tmp.csv")))
tmp_py <- exp(tmp_py)
dim(tmp_py) <- c(length(tmp_py), 1)
class(tmp_py) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
loo(llfun_logistic, draws = parameter_draws_1, data = stan_df_1, r_eff = r_eff)
View(stan_df_1)
x_i <- as.matrix(stan_df_1[, which(grepl(colnames(stan_df_1), pattern = "X")), drop=FALSE])
dim(x_i)
logit_pred <- parameter_draws_1 %*% t(x_i)
dim(logit_pred)
dim(t(x_i))
tmp <-  dbinom(x = stan_df_1$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
dim(tmp)
head(tmp)
View(tmp)
dim(tmp)
?dbinom
View(logit_pred)
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
rm(list = ls())
