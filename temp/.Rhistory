# here we define compute_lpds and related function (lpd_i, logMeanExp) so that we can
# reproduce the elpd_loo_approximation part, which returns a vector of lpds
# https://github.com/stan-dev/loo/blob/e197aa7c5ac56881ad67dae3f90479af96d83c0a/R/helpers.R#L13
logMeanExp <- function(x) {
logS <- log(length(x))
matrixStats::logSumExp(x) - logS
}
lpd_i <- function(i, llfun, data, draws) {
ll_i <- llfun(data_i = data[i,, drop=FALSE], draws = draws)
ll_i <- as.vector(ll_i)
lpd_i <- logMeanExp(ll_i)
lpd_i
}
compute_lpds <- function(N, data, draws, llfun, cores) {
if (cores == 1) {
lpds <- lapply(X = seq_len(N), FUN = lpd_i, llfun, data, draws)
} else {
if (.Platform$OS.type != "windows") {
lpds <- parallel::mclapply(X = seq_len(N), mc.cores = cores, FUN = lpd_i, llfun, data, draws)
} else {
cl <- makePSOCKcluster(cores)
on.exit(stopCluster(cl))
lpds <- parLapply(cl, X = seq_len(N), fun = lpd_i, llfun, data, draws)
}
}
unlist(lpds)
}
elpd_loo_approx <- compute_lpds(N, data, point_est, llfun_logistic, cores=1)
elpd_loo_approximation <- elpd_loo_approx
# understand order()
set.seed(123)
tmp2 <- stats::runif(10)
order(tmp2)
tmp2[order(tmp2)]
#
subsample_idxs <- function(estimator, elpd_loo_approximation, observations) {
if (estimator == "hh_pps") {
pi_values <- pps_elpd_loo_approximation_to_pis(elpd_loo_approximation)
idxs_df <- pps_sample(observations, pis = pi_values)
}
if (estimator == "diff_srs" | estimator == "srs") {
if (observations > length(elpd_loo_approximation)) {
stop("'observations' is larger than the total sample size in 'data'.", call. = FALSE)
}
idx <- 1:length(elpd_loo_approximation)
# order function here is not the real ranking of data, but how to get those data so that the new a[order(a)] is ascending.
idx_m <- idx[order(stats::runif(length(elpd_loo_approximation)))][1:observations]
idx_m <- idx_m[order(idx_m)]
idxs_df <- data.frame(idx=as.integer(idx_m), m_i=1L)
}
# assert_subsample_idxs(x = idxs_df)
idxs_df
}
observations <- 100
idxs <- subsample_idxs(
estimator = "diff_srs", # here the default is simple random sampling (srs), i.e. randomly sub-sample
elpd_loo_approximation = elpd_loo_approx,
observations = observations)
View(idxs)
data_subsample <- data[idxs$idx,, drop = FALSE]
if (length(r_eff) > 1) {
r_eff <- r_eff[idxs$idx]
}
loo_obj <- loo.function(
x = llfun_logistic,
data = data_subsample,
draws = draws,
r_eff = r_eff,
#save_psis = save_psis,
#cores = cores
)
loo_obj
loo_1
View(loo_1)
View(loo_obj)
View(loo_ss_1)
loo_ss_1
library(tidyverse)
library(rstan)
library(loo)
dim(loo_ss_1$pointwise)
head(loo_ss_1$pointwise)
head(loo_obj$pointwise)
loo_ss_1$loo_subsampling$elpd_loo_approx[32]
loo_ss_1$loo_subsampling$elpd_loo_approx[75]
loo_ss_1$estimates
rm(list = ls())
install.packages(c("binom", "bruceR", "emmeans", "lmtest", "openssl", "rms", "robustbase", "V8", "zip"))
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
View(wells)
write.csv(wells, file = "wells.csv")
write.csv(wells, file = "wells.csv", row.names = F)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
View(standata)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
loo(fit_1)
?loo::loo
loo(fit_1, llfun_logistic)
loo(llfun_logistic, draws = parameter_draws_1, data = stan_df_1, r_eff = r_eff)
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
?relative_eff
data_i <- stan_df_1[2, , drop = TRUE]
View(data_i)
tmp <- llfun_logistic(data_i, draws = parameter_draws_1)
data_i <- stan_df_1[2, , drop = FALSE]
tmp <- llfun_logistic(data_i, draws = parameter_draws_1)
?apply
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
r_eff
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
chain_id <- rep(1:4, each=1000)
data <- stan_df_1
draws <- parameter_draws_1
x <- llfun_logistic
dim(x)
c(length(x), 1)
dim(x) <- c(length(x), 1)
N <- dim(data)[1]
n_eff_list <-
lapply(
X = seq_len(N),
FUN = function(i) {
val_i <- llfun_logistic(data_i = data[i, , drop = FALSE], draws = draws, ...)
relative_eff.default(as.vector(val_i), chain_id = chain_id, cores = 1)
}
)
val_i <- llfun_logistic(data_i = data[i, , drop = FALSE], draws = draws)
n_eff_list <-
lapply(
X = seq_len(N),
FUN = function(i) {
val_i <- llfun_logistic(data_i = data[i, , drop = FALSE], draws = draws)
relative_eff.default(as.vector(val_i), chain_id = chain_id, cores = 1)
}
)
n_eff_list <-
lapply(
X = seq_len(N),
FUN = function(i) {
val_i <- llfun_logistic(data_i = data[i, , drop = FALSE], draws = draws)
#relative_eff.default(as.vector(val_i), chain_id = chain_id, cores = 1)
}
)
View(n_eff_list)
data_i <- data[1, , drop = FALSE]
llfun_logistic(data_i = data_i, draws = draws)
tmp <- llfun_logistic(data_i = data_i, draws = draws)
View(tmp)
mean(tmp)
data_2 <- data[2, , drop = FALSE]
tmp2 <- llfun_logistic(data_2 = data_i, draws = draws)
mean(tmp2)
tmp2 <- llfun_logistic(data_i = data_2, draws = draws)
mean(tmp2)
tmp_ <- as.vector(tmp)
dim(tmp_) <- c(length(tmp_), 1)
class(tmp_) <- "matrix"
relative_eff.matrix(tmp_, chain_id)
tmp_0 <- llmatrix_to_array(tmp_, chain_id)
llmatrix_to_array <- function(x, chain_id) {
stopifnot(is.matrix(x), all(chain_id == as.integer(chain_id)))
lldim <- dim(x)
n_chain <- length(unique(chain_id))
chain_id <- as.integer(chain_id)
chain_counts <- as.numeric(table(chain_id))
if (length(chain_id) != lldim[1]) {
stop("Number of rows in matrix not equal to length(chain_id).",
call. = FALSE)
} else if (any(chain_counts != chain_counts[1])) {
stop("Not all chains have same number of iterations.",
call. = FALSE)
} else if (max(chain_id) != n_chain) {
stop("max(chain_id) not equal to the number of chains.",
call. = FALSE)
}
n_iter <- lldim[1] / n_chain
n_obs <- lldim[2]
a <- array(data = NA, dim = c(n_iter, n_chain, n_obs))
for (c in seq_len(n_chain)) {
a[, c, ] <- x[chain_id == c, , drop = FALSE]
}
return(a)
}
tmp_0 <- llmatrix_to_array(tmp_, chain_id)
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
ess_rfun <- function(sims) {
# Compute the effective sample size for samples of several chains
# for one parameter; see the C++ code of function
# effective_sample_size in chains.cpp
#
# Args:
#   sims: a 2-d array _without_ warmup samples (# iter * # chains)
#
if (is.vector(sims)) dim(sims) <- c(length(sims), 1)
chains <- ncol(sims)
n_samples <- nrow(sims)
acov <- lapply(1:chains, FUN = function(i) autocovariance(sims[,i]))
acov <- do.call(cbind, acov)
chain_mean <- colMeans(sims)
mean_var <- mean(acov[1,]) * n_samples / (n_samples - 1)
var_plus <- mean_var * (n_samples - 1) / n_samples
if (chains > 1)
var_plus <- var_plus + var(chain_mean)
# Geyer's initial positive sequence
rho_hat_t <- rep.int(0, n_samples)
t <- 0
rho_hat_even <- 1
rho_hat_t[t + 1] <- rho_hat_even
rho_hat_odd <- 1 - (mean_var - mean(acov[t + 2, ])) / var_plus
rho_hat_t[t + 2] <- rho_hat_odd
while (t < nrow(acov) - 5 && !is.nan(rho_hat_even + rho_hat_odd) &&
(rho_hat_even + rho_hat_odd > 0)) {
t <- t + 2
rho_hat_even = 1 - (mean_var - mean(acov[t + 1, ])) / var_plus
rho_hat_odd = 1 - (mean_var - mean(acov[t + 2, ])) / var_plus
if ((rho_hat_even + rho_hat_odd) >= 0) {
rho_hat_t[t + 1] <- rho_hat_even
rho_hat_t[t + 2] <- rho_hat_odd
}
}
max_t <- t
# this is used in the improved estimate
if (rho_hat_even>0)
rho_hat_t[max_t + 1] <- rho_hat_even
# Geyer's initial monotone sequence
t <- 0
while (t <= max_t - 4) {
t <- t + 2
if (rho_hat_t[t + 1] + rho_hat_t[t + 2] >
rho_hat_t[t - 1] + rho_hat_t[t]) {
rho_hat_t[t + 1] = (rho_hat_t[t - 1] + rho_hat_t[t]) / 2;
rho_hat_t[t + 2] = rho_hat_t[t + 1];
}
}
ess <- chains * n_samples
# Geyer's truncated estimate
# tau_hat <- -1 + 2 * sum(rho_hat_t[1:max_t])
# Improved estimate reduces variance in antithetic case
tau_hat <- -1 + 2 * sum(rho_hat_t[1:max_t]) + rho_hat_t[max_t+1]
# Safety check for negative values and with max ess equal to ess*log10(ess)
tau_hat <- max(tau_hat, 1/log10(ess))
ess <- ess / tau_hat
ess
}
n_eff_vec <- apply(x, 3, ess_rfun)
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
autocovariance <- function(y) {
# Compute autocovariance estimates for every lag for the specified
# input sequence using a fast Fourier transform approach.
N <- length(y)
M <- fft_next_good_size(N)
Mt2 <- 2 * M
yc <- y - mean(y)
yc <- c(yc, rep.int(0, Mt2-N))
transform <- stats::fft(yc)
ac <- stats::fft(Conj(transform) * transform, inverse = TRUE)
# use "biased" estimate as recommended by Geyer (1992)
ac <- Re(ac)[1:N] / (N^2 * 2)
ac
}
fft_next_good_size <- function(N) {
# Find the optimal next size for the FFT so that
# a minimum number of zeros are padded.
if (N <= 2)
return(2)
while (TRUE) {
m = N
while ((m %% 2) == 0) m = m / 2
while ((m %% 3) == 0) m = m / 3
while ((m %% 5) == 0) m = m / 5
if (m <= 1)
return(N)
N = N + 1
}
}
ess_rfun <- function(sims) {
# Compute the effective sample size for samples of several chains
# for one parameter; see the C++ code of function
# effective_sample_size in chains.cpp
#
# Args:
#   sims: a 2-d array _without_ warmup samples (# iter * # chains)
#
if (is.vector(sims)) dim(sims) <- c(length(sims), 1)
chains <- ncol(sims)
n_samples <- nrow(sims)
acov <- lapply(1:chains, FUN = function(i) autocovariance(sims[,i]))
acov <- do.call(cbind, acov)
chain_mean <- colMeans(sims)
mean_var <- mean(acov[1,]) * n_samples / (n_samples - 1)
var_plus <- mean_var * (n_samples - 1) / n_samples
if (chains > 1)
var_plus <- var_plus + var(chain_mean)
# Geyer's initial positive sequence
rho_hat_t <- rep.int(0, n_samples)
t <- 0
rho_hat_even <- 1
rho_hat_t[t + 1] <- rho_hat_even
rho_hat_odd <- 1 - (mean_var - mean(acov[t + 2, ])) / var_plus
rho_hat_t[t + 2] <- rho_hat_odd
while (t < nrow(acov) - 5 && !is.nan(rho_hat_even + rho_hat_odd) &&
(rho_hat_even + rho_hat_odd > 0)) {
t <- t + 2
rho_hat_even = 1 - (mean_var - mean(acov[t + 1, ])) / var_plus
rho_hat_odd = 1 - (mean_var - mean(acov[t + 2, ])) / var_plus
if ((rho_hat_even + rho_hat_odd) >= 0) {
rho_hat_t[t + 1] <- rho_hat_even
rho_hat_t[t + 2] <- rho_hat_odd
}
}
max_t <- t
# this is used in the improved estimate
if (rho_hat_even>0)
rho_hat_t[max_t + 1] <- rho_hat_even
# Geyer's initial monotone sequence
t <- 0
while (t <= max_t - 4) {
t <- t + 2
if (rho_hat_t[t + 1] + rho_hat_t[t + 2] >
rho_hat_t[t - 1] + rho_hat_t[t]) {
rho_hat_t[t + 1] = (rho_hat_t[t - 1] + rho_hat_t[t]) / 2;
rho_hat_t[t + 2] = rho_hat_t[t + 1];
}
}
ess <- chains * n_samples
# Geyer's truncated estimate
# tau_hat <- -1 + 2 * sum(rho_hat_t[1:max_t])
# Improved estimate reduces variance in antithetic case
tau_hat <- -1 + 2 * sum(rho_hat_t[1:max_t]) + rho_hat_t[max_t+1]
# Safety check for negative values and with max ess equal to ess*log10(ess)
tau_hat <- max(tau_hat, 1/log10(ess))
ess <- ess / tau_hat
ess
}
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
mean(r_eff)
data_2 <- data[2, , drop = FALSE]
tmp2 <- llfun_logistic(data_i = data_2, draws = draws)
tmp_ <- as.vector(tmp2)
dim(tmp_) <- c(length(tmp_), 1)
class(tmp_) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
# import the data from python
tmp_py <- read.csv("tmp.csv")
View(tmp_py)
# import the data from python
tmp_py <- read.csv("tmp.csv")
# import the data from python
tmp_py <- read.csv("tmp.csv")
View(tmp_py)
# import the data from python
tmp_py <- t(read.csv("tmp.csv"))
# import the data from python
tmp_py <- as.vector(t(read.csv("tmp.csv")))
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
dim(tmp_py) <- c(length(tmp_py), 1)
class(tmp_py) <- "matrix"
tmp_0 <- llmatrix_to_array(tmp_py, chain_id) # from matrix to 1000 * 4 * 3020 array
length(dim(tmp_0)) == 3
S <- prod(dim(tmp_0)[1:2])
n_eff_vec <- apply(tmp_0, 3, ess_rfun)
n_eff_vec/S
