library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
# print(loo_ss_1)
loo_ss_1
loo_1 <- loo(fit_1)
?
loo
loo_1
loo_1 <- loo(    llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
loo_1
set.seed(4711)
loo_1 <- loo(llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
loo_1
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
end.time <- Sys.time()
print("Time taken: ", end.time - start.time)
print(paste("Time taken: ", end.time - start.time))
start.time <- Sys.time()
set.seed(4711)
loo_1 <- loo(llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
end.time <- Sys.time()
print(paste("Time taken: ", end.time - start.time))
loo_1
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
end.time <- Sys.time()
print(paste("Time taken: ", end.time - start.time))
loo_ss_1
loo_approximation <- 'plpd'
draws <- parameter_draws_1  # 4000 * 3
data <- stan_df_1
llfun <- llfun_logistic
N <- dim(data)[1]
estimator <- "diff_srs"
loo_approximation <- 'plpd'
observations <- 100
# here we replace the function .compute_point_estimate.matrix, which basically compute the means of parameters and transpose
tmp_compute_point_estimate <- function(draws) {
t(as.matrix(colMeans(draws)))
}
point_est <- tmp_compute_point_estimate(draws)
?colMeans
colMeans(draws)
mean(draws[,1])
dim(as.matrixcolMeans(draws)))
dim(as.matrixcolMeans(draws))
dim(as.matrix(colMeans(draws)))
##  elpd_loo_approx <- elpd_loo_approximation ()...
# draws <- .thin_draws(draws, loo_approximation_draws)
# here we define compute_lpds and related function (lpd_i, logMeanExp) so that we can
# reproduce the elpd_loo_approximation part, which returns a vector of lpds
# https://github.com/stan-dev/loo/blob/e197aa7c5ac56881ad67dae3f90479af96d83c0a/R/helpers.R#L13
logMeanExp <- function(x) {
logS <- log(length(x))
matrixStats::logSumExp(x) - logS
}
lpd_i <- function(i, llfun, data, draws) {
ll_i <- llfun(data_i = data[i,, drop=FALSE], draws = draws)
ll_i <- as.vector(ll_i)
lpd_i <- logMeanExp(ll_i)
lpd_i
}
compute_lpds <- function(N, data, draws, llfun, cores) {
if (cores == 1) {
lpds <- lapply(X = seq_len(N), FUN = lpd_i, llfun, data, draws)
} else {
if (.Platform$OS.type != "windows") {
lpds <- parallel::mclapply(X = seq_len(N), mc.cores = cores, FUN = lpd_i, llfun, data, draws)
} else {
cl <- makePSOCKcluster(cores)
on.exit(stopCluster(cl))
lpds <- parLapply(cl, X = seq_len(N), fun = lpd_i, llfun, data, draws)
}
}
unlist(lpds)
}
elpd_loo_approx <- compute_lpds(N, data, point_est, llfun_logistic, cores=1)
elpd_loo_approximation <- elpd_loo_approx
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_2 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
loo_approximation = 'tis',
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_2 <- end.time <- Sys.time()
print(paste("Time taken: ", time_loo_ss_2))
loo_ss_2
time_loo_ss_2 <- Sys.time() - start.time
time_loo_ss_2
print(paste("Time taken: ", time_loo_ss_2, "secs"))
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_1 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_ss_1, "secs"))
loo_ss_1
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_2 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
loo_approximation = 'tis',
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_2 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_ss_2, "secs"))
loo_ss_2
start.time <- Sys.time()
set.seed(4711)
loo_1 <- loo(llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
time_loo_1 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_1, "secs"))
loo_1
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_1 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_ss_1, "secs"))
loo_ss_1
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_2 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
loo_approximation = 'tis',
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_2 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_ss_2, "secs"))
loo_ss_2
start.time <- Sys.time()
set.seed(4711)
loo_1 <- loo(llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
time_loo_1 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_1, "secs"))
loo_1
# used for draws argument to loo_i
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
i <- 1
data_i <- stan_df_1[i, ]    # must be a dataframe
draws <- parameter_draws_1  # 4000 * 3
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])  # get the parameters 1*3
logit_pred <- draws %*% t(x_i) # matrix multiplication 4000 * 3 %*% 3 * 1
tmp <- dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
draws <- parameter_draws_1  # 4000 * 3
data <- stan_df_1
llfun <- llfun_logistic
N <- dim(data)[1]
estimator <- "diff_srs"
loo_approximation <- 'plpd'
observations <- 100
# here we replace the function .compute_point_estimate.matrix, which basically compute the means of parameters and transpose
tmp_compute_point_estimate <- function(draws) {
t(as.matrix(colMeans(draws)))
}
point_est <- tmp_compute_point_estimate(draws)
# here we replace the function .compute_point_estimate.matrix, which basically compute the means of parameters and transpose
tmp_compute_point_estimate <- function(draws) {
t(as.matrix(colMeans(draws)))
}
point_est <- tmp_compute_point_estimate(draws)
##  elpd_loo_approx <- elpd_loo_approximation ()...
# draws <- .thin_draws(draws, loo_approximation_draws)
# here we define compute_lpds and related function (lpd_i, logMeanExp) so that we can
# reproduce the elpd_loo_approximation part, which returns a vector of lpds
# https://github.com/stan-dev/loo/blob/e197aa7c5ac56881ad67dae3f90479af96d83c0a/R/helpers.R#L13
logMeanExp <- function(x) {
logS <- log(length(x))
matrixStats::logSumExp(x) - logS
}
lpd_i <- function(i, llfun, data, draws) {
ll_i <- llfun(data_i = data[i,, drop=FALSE], draws = draws)
ll_i <- as.vector(ll_i)
lpd_i <- logMeanExp(ll_i)
lpd_i
}
compute_lpds <- function(N, data, draws, llfun, cores) {
if (cores == 1) {
lpds <- lapply(X = seq_len(N), FUN = lpd_i, llfun, data, draws)
} else {
if (.Platform$OS.type != "windows") {
lpds <- parallel::mclapply(X = seq_len(N), mc.cores = cores, FUN = lpd_i, llfun, data, draws)
} else {
cl <- makePSOCKcluster(cores)
on.exit(stopCluster(cl))
lpds <- parLapply(cl, X = seq_len(N), fun = lpd_i, llfun, data, draws)
}
}
unlist(lpds)
}
elpd_loo_approx <- compute_lpds(N, data, point_est, llfun_logistic, cores=1)
elpd_loo_approximation <- elpd_loo_approx
# understand order()
set.seed(123)
tmp2 <- stats::runif(10)
order(tmp2)
tmp2[order(tmp2)]
#
subsample_idxs <- function(estimator, elpd_loo_approximation, observations) {
if (estimator == "hh_pps") {
pi_values <- pps_elpd_loo_approximation_to_pis(elpd_loo_approximation)
idxs_df <- pps_sample(observations, pis = pi_values)
}
if (estimator == "diff_srs" | estimator == "srs") {
if (observations > length(elpd_loo_approximation)) {
stop("'observations' is larger than the total sample size in 'data'.", call. = FALSE)
}
idx <- 1:length(elpd_loo_approximation)
# order function here is not the real ranking of data, but how to get those data so that the new a[order(a)] is ascending.
idx_m <- idx[order(stats::runif(length(elpd_loo_approximation)))][1:observations]
idx_m <- idx_m[order(idx_m)]
idxs_df <- data.frame(idx=as.integer(idx_m), m_i=1L)
}
# assert_subsample_idxs(x = idxs_df)
idxs_df
}
observations <- 100
idxs <- subsample_idxs(
estimator = "diff_srs", # here the default is simple random sampling (srs), i.e. randomly sub-sample
elpd_loo_approximation = elpd_loo_approx,
observations = observations)
View(idxs)
data_subsample <- data[idxs$idx,, drop = FALSE]
if (length(r_eff) > 1) {
r_eff <- r_eff[idxs$idx]
}
loo_obj <- loo.function(
x = llfun_logistic,
data = data_subsample,
draws = draws,
r_eff = r_eff,
#save_psis = save_psis,
#cores = cores
)
loo_obj
loo_1
View(loo_1)
View(loo_obj)
View(loo_ss_1)
loo_ss_1
library(tidyverse)
library(rstan)
library(loo)
dim(loo_ss_1$pointwise)
head(loo_ss_1$pointwise)
head(loo_obj$pointwise)
loo_ss_1$loo_subsampling$elpd_loo_approx[32]
loo_ss_1$loo_subsampling$elpd_loo_approx[75]
loo_ss_1$estimates
rm(list = ls())
install.packages(c("binom", "bruceR", "emmeans", "lmtest", "openssl", "rms", "robustbase", "V8", "zip"))
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
View(wells)
write.csv(wells, file = "wells.csv")
write.csv(wells, file = "wells.csv", row.names = F)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
View(standata)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
loo(fit_1)
?loo::loo
loo(fit_1, llfun_logistic)
loo(llfun_logistic, draws = parameter_draws_1, data = stan_df_1, r_eff = r_eff)
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
# write.csv(wells, file = "wells.csv", row.names = F)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
?relative_eff
data_i <- stan_df_1[2, , drop = TRUE]
View(data_i)
tmp <- llfun_logistic(data_i, draws = parameter_draws_1)
data_i <- stan_df_1[2, , drop = FALSE]
tmp <- llfun_logistic(data_i, draws = parameter_draws_1)
?apply
