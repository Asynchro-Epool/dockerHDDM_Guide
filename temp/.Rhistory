pi_values <- pps_elpd_loo_approximation_to_pis(elpd_loo_approximation)
idxs_df <- pps_sample(observations, pis = pi_values)
}
if (estimator == "diff_srs" | estimator == "srs") {
if (observations > length(elpd_loo_approximation)) {
stop("'observations' is larger than the total sample size in 'data'.", call. = FALSE)
}
idx <- 1:length(elpd_loo_approximation)
# order function here is not the real ranking of data, but how to get those data so that the new a[order(a)] is ascending.
idx_m <- idx[order(stats::runif(length(elpd_loo_approximation)))][1:observations]
idx_m <- idx_m[order(idx_m)]
idxs_df <- data.frame(idx=as.integer(idx_m), m_i=1L)
}
# assert_subsample_idxs(x = idxs_df)
idxs_df
}
observations <- 100
idxs <- subsample_idxs(
estimator = "diff_srs", # here the default is simple random sampling (srs), i.e. randomly sub-sample
elpd_loo_approximation = elpd_loo_approx,
observations = observations)
data_subsample <- data[idxs$idx,, drop = FALSE]
if (length(r_eff) > 1) {
r_eff <- r_eff[idxs$idx]
}
loo_obj <- loo.function(
x = llfun_logistic,
data = data_subsample,
draws = draws,
r_eff = r_eff,
#save_psis = save_psis,
#cores = cores
)
View(loo_obj)
View(loo_obj)
loo_obj$pointwise
loo_obj$pointwise[,"idx"]
x_tmp <- loo_obj
x_tmp$pointwise <- add_subsampling_vars_to_pointwise(pointwise = x_tmp$pointwise, idxs=idxs, elpd_loo_approx= elpd_loo_approx)
# The third layer functions
add_subsampling_vars_to_pointwise <- function(pointwise, idxs, elpd_loo_approx) {
checkmate::assert_matrix(pointwise,
any.missing = FALSE,
min.cols = 5)
checkmate::assert_names(colnames(pointwise), identical.to = c("elpd_loo","mcse_elpd_loo","p_loo","looic", "influence_pareto_k"))
# assert_subsample_idxs(idxs)
checkmate::assert_numeric(elpd_loo_approx)
pw <- cbind(as.data.frame(pointwise), idxs)
pw$elpd_loo_approx <- elpd_loo_approx[idxs$idx]
pw <- as.matrix(pw)
rownames(pw) <- NULL
#assert_subsampling_pointwise(pw)
pw
}
x_tmp$pointwise <- add_subsampling_vars_to_pointwise(pointwise = x_tmp$pointwise, idxs=idxs, elpd_loo_approx= elpd_loo_approx)
View(x_tmp)
colnames(x_tmp$pointwise)
x_tmp$pointwise$idx
x_tmp$pointwise[,"idx"]
colnames(loo_obj$pointwise)
idxs
View(x_tmp)
head(x_tmp$pointwise)
loo_approximation
loo_approximation_draws
estimator
.llfun
data_dim
ndraws
dim(data)
.ndraws(draws)
x_tmp$loo_subsampling <- list()
x_tmp$loo_subsampling$elpd_loo_approx <- elpd_loo_approx
x_tmp$loo_subsampling$loo_approximation <- loo_approximation
x_tmp$loo_subsampling["loo_approximation_draws"] <- list(NULL)
x_tmp$loo_subsampling$estimator <- estimator
x_tmp$loo_subsampling$.llfun <- llfun
x_tmp$loo_subsampling[".llgrad"] <- list(NULL)
x_tmp$loo_subsampling[".llhess"] <- list(NULL)
x_tmp$loo_subsampling$data_dim <- dim(data)
x_tmp$loo_subsampling$ndraws <- .ndraws(draws)
View(x_tmp)
View(loo_ss_1)
## core fucntion of srs_diff_est
y_approx_tmp <- x_tmp$loo_subsampling$elpd_loo_approx
y_tmp <- x_tmp$pointwise[, 'elpd_loo']
y_idx_tmp <- x_tmp$pointwise[, "idx"]
N <- length(y_approx)
N <- length(y_approx_tmp)
m <- length(y_tmp)
y_approx_m <- y_approx_tmp[y_idx_tmp]
e_i <- y_tmp - y_approx_m
t_pi_tilde <- sum(y_approx_tmp)
t_pi2_tilde <- sum(y_approx_tmp^2)
t_e <- N * mean(e_i)
t_hat_epsilon <- N * mean(y_tmp^2 - y_approx_m^2)
est_list <- list(m = length(y_tmp), N = N)
est_list$y_hat <- t_pi_tilde + t_e
est_list$v_y_hat <- N^2 * (1 - m / N) * var(e_i) / m
est_list$hat_v_y <- (t_pi2_tilde + t_hat_epsilon) - # a (has been checked)
(1/N) * (t_e^2 - est_list$v_y_hat + 2 * t_pi_tilde * est_list$y_hat - t_pi_tilde^2) # b
est_list
elpd_loo_est <- est_list
x_tmp$estimates
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
# print(loo_ss_1)
loo_ss_1
loo_1 <- loo(fit_1)
?
loo
loo_1
loo_1 <- loo(    llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
loo_1
set.seed(4711)
loo_1 <- loo(llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
loo_1
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
end.time <- Sys.time()
print("Time taken: ", end.time - start.time)
print(paste("Time taken: ", end.time - start.time))
start.time <- Sys.time()
set.seed(4711)
loo_1 <- loo(llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
end.time <- Sys.time()
print(paste("Time taken: ", end.time - start.time))
loo_1
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
end.time <- Sys.time()
print(paste("Time taken: ", end.time - start.time))
loo_ss_1
loo_approximation <- 'plpd'
draws <- parameter_draws_1  # 4000 * 3
data <- stan_df_1
llfun <- llfun_logistic
N <- dim(data)[1]
estimator <- "diff_srs"
loo_approximation <- 'plpd'
observations <- 100
# here we replace the function .compute_point_estimate.matrix, which basically compute the means of parameters and transpose
tmp_compute_point_estimate <- function(draws) {
t(as.matrix(colMeans(draws)))
}
point_est <- tmp_compute_point_estimate(draws)
?colMeans
colMeans(draws)
mean(draws[,1])
dim(as.matrixcolMeans(draws)))
dim(as.matrixcolMeans(draws))
dim(as.matrix(colMeans(draws)))
##  elpd_loo_approx <- elpd_loo_approximation ()...
# draws <- .thin_draws(draws, loo_approximation_draws)
# here we define compute_lpds and related function (lpd_i, logMeanExp) so that we can
# reproduce the elpd_loo_approximation part, which returns a vector of lpds
# https://github.com/stan-dev/loo/blob/e197aa7c5ac56881ad67dae3f90479af96d83c0a/R/helpers.R#L13
logMeanExp <- function(x) {
logS <- log(length(x))
matrixStats::logSumExp(x) - logS
}
lpd_i <- function(i, llfun, data, draws) {
ll_i <- llfun(data_i = data[i,, drop=FALSE], draws = draws)
ll_i <- as.vector(ll_i)
lpd_i <- logMeanExp(ll_i)
lpd_i
}
compute_lpds <- function(N, data, draws, llfun, cores) {
if (cores == 1) {
lpds <- lapply(X = seq_len(N), FUN = lpd_i, llfun, data, draws)
} else {
if (.Platform$OS.type != "windows") {
lpds <- parallel::mclapply(X = seq_len(N), mc.cores = cores, FUN = lpd_i, llfun, data, draws)
} else {
cl <- makePSOCKcluster(cores)
on.exit(stopCluster(cl))
lpds <- parLapply(cl, X = seq_len(N), fun = lpd_i, llfun, data, draws)
}
}
unlist(lpds)
}
elpd_loo_approx <- compute_lpds(N, data, point_est, llfun_logistic, cores=1)
elpd_loo_approximation <- elpd_loo_approx
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_2 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
loo_approximation = 'tis',
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_2 <- end.time <- Sys.time()
print(paste("Time taken: ", time_loo_ss_2))
loo_ss_2
time_loo_ss_2 <- Sys.time() - start.time
time_loo_ss_2
print(paste("Time taken: ", time_loo_ss_2, "secs"))
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_1 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_ss_1, "secs"))
loo_ss_1
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_2 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
loo_approximation = 'tis',
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_2 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_ss_2, "secs"))
loo_ss_2
start.time <- Sys.time()
set.seed(4711)
loo_1 <- loo(llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
time_loo_1 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_1, "secs"))
loo_1
rm(list = ls())
library(tidyverse)
library(rstan)
library(loo)
# we'll add an argument log to toggle whether this is a log-likelihood or
# likelihood function. this will be useful later in the vignette.
llfun_logistic <- function(data_i, draws, log = TRUE) {
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
logit_pred <- draws %*% t(x_i)
dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
}
# Prepare data
# url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
# wells <- read.table(url)
# save(wells, file = "wells.Rdata")
# wells$dist100 <- with(wells, dist / 100)
load("wells.Rdata")
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))
# Compile
stan_mod <- stan_model("logistic.stan")
# Fit model
fit_1 <- sampling(stan_mod, data = standata, seed = 4711, cores = 4)
print(fit_1, pars = "beta")
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
r_eff <- relative_eff(llfun_logistic,
log = FALSE, # relative_eff wants likelihood not log-likelihood values
chain_id = rep(1:4, each = 1000),
data = stan_df_1,
draws = parameter_draws_1,
cores = 4)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_1 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_1 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_ss_1, "secs"))
loo_ss_1
start.time <- Sys.time()
# subsampling
set.seed(4711)
loo_ss_2 <-
loo_subsample(
llfun_logistic,
observations = 100, # take a subsample of size 100
cores = 2,
# these next objects were computed above
loo_approximation = 'tis',
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1
)
time_loo_ss_2 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_ss_2, "secs"))
loo_ss_2
start.time <- Sys.time()
set.seed(4711)
loo_1 <- loo(llfun_logistic,
cores = 4,
# these next objects were computed above
r_eff = r_eff,
draws = parameter_draws_1,
data = stan_df_1)
time_loo_1 <- Sys.time() - start.time
print(paste("Time taken: ", time_loo_1, "secs"))
loo_1
# used for draws argument to loo_i
parameter_draws_1 <- extract(fit_1)$beta
# used for data argument to loo_i
stan_df_1 <- as.data.frame(standata)
i <- 1
data_i <- stan_df_1[i, ]    # must be a dataframe
draws <- parameter_draws_1  # 4000 * 3
x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])  # get the parameters 1*3
logit_pred <- draws %*% t(x_i) # matrix multiplication 4000 * 3 %*% 3 * 1
tmp <- dbinom(x = data_i$y, size = 1, prob = 1/(1 + exp(-logit_pred)), log = log)
draws <- parameter_draws_1  # 4000 * 3
data <- stan_df_1
llfun <- llfun_logistic
N <- dim(data)[1]
estimator <- "diff_srs"
loo_approximation <- 'plpd'
observations <- 100
# here we replace the function .compute_point_estimate.matrix, which basically compute the means of parameters and transpose
tmp_compute_point_estimate <- function(draws) {
t(as.matrix(colMeans(draws)))
}
point_est <- tmp_compute_point_estimate(draws)
# here we replace the function .compute_point_estimate.matrix, which basically compute the means of parameters and transpose
tmp_compute_point_estimate <- function(draws) {
t(as.matrix(colMeans(draws)))
}
point_est <- tmp_compute_point_estimate(draws)
##  elpd_loo_approx <- elpd_loo_approximation ()...
# draws <- .thin_draws(draws, loo_approximation_draws)
# here we define compute_lpds and related function (lpd_i, logMeanExp) so that we can
# reproduce the elpd_loo_approximation part, which returns a vector of lpds
# https://github.com/stan-dev/loo/blob/e197aa7c5ac56881ad67dae3f90479af96d83c0a/R/helpers.R#L13
logMeanExp <- function(x) {
logS <- log(length(x))
matrixStats::logSumExp(x) - logS
}
lpd_i <- function(i, llfun, data, draws) {
ll_i <- llfun(data_i = data[i,, drop=FALSE], draws = draws)
ll_i <- as.vector(ll_i)
lpd_i <- logMeanExp(ll_i)
lpd_i
}
compute_lpds <- function(N, data, draws, llfun, cores) {
if (cores == 1) {
lpds <- lapply(X = seq_len(N), FUN = lpd_i, llfun, data, draws)
} else {
if (.Platform$OS.type != "windows") {
lpds <- parallel::mclapply(X = seq_len(N), mc.cores = cores, FUN = lpd_i, llfun, data, draws)
} else {
cl <- makePSOCKcluster(cores)
on.exit(stopCluster(cl))
lpds <- parLapply(cl, X = seq_len(N), fun = lpd_i, llfun, data, draws)
}
}
unlist(lpds)
}
elpd_loo_approx <- compute_lpds(N, data, point_est, llfun_logistic, cores=1)
elpd_loo_approximation <- elpd_loo_approx
# understand order()
set.seed(123)
tmp2 <- stats::runif(10)
order(tmp2)
tmp2[order(tmp2)]
#
subsample_idxs <- function(estimator, elpd_loo_approximation, observations) {
if (estimator == "hh_pps") {
pi_values <- pps_elpd_loo_approximation_to_pis(elpd_loo_approximation)
idxs_df <- pps_sample(observations, pis = pi_values)
}
if (estimator == "diff_srs" | estimator == "srs") {
if (observations > length(elpd_loo_approximation)) {
stop("'observations' is larger than the total sample size in 'data'.", call. = FALSE)
}
idx <- 1:length(elpd_loo_approximation)
# order function here is not the real ranking of data, but how to get those data so that the new a[order(a)] is ascending.
idx_m <- idx[order(stats::runif(length(elpd_loo_approximation)))][1:observations]
idx_m <- idx_m[order(idx_m)]
idxs_df <- data.frame(idx=as.integer(idx_m), m_i=1L)
}
# assert_subsample_idxs(x = idxs_df)
idxs_df
}
observations <- 100
idxs <- subsample_idxs(
estimator = "diff_srs", # here the default is simple random sampling (srs), i.e. randomly sub-sample
elpd_loo_approximation = elpd_loo_approx,
observations = observations)
View(idxs)
data_subsample <- data[idxs$idx,, drop = FALSE]
if (length(r_eff) > 1) {
r_eff <- r_eff[idxs$idx]
}
loo_obj <- loo.function(
x = llfun_logistic,
data = data_subsample,
draws = draws,
r_eff = r_eff,
#save_psis = save_psis,
#cores = cores
)
loo_obj
loo_1
View(loo_1)
View(loo_obj)
View(loo_ss_1)
loo_ss_1
