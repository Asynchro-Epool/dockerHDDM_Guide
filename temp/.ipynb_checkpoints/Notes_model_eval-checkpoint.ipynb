{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on model evaluation so far\n",
    "\n",
    "These notes are generated when I was trying to convert the `HDDM` object to a `arviz` `InferenceData`.\n",
    "\n",
    "This process is very painful to me, because basically I relied on the internet for searching different sources. I've posted some questions on some forums/issues, but none of them were really replied. Fortunately, I am an old internet bag and can put information from different sources together and find hint from here and there. \n",
    "\n",
    "## Quantitative indices\n",
    "\n",
    "### $R^2$ and Explained variance\n",
    "$R^2$ is a goodness-of-fit measure that tells you how well the data fits the model that we created. Put it more simply,  it explains the proportion of variance in the outcomes that the independenct variables explain.\n",
    "\n",
    "### How to calculate $R^2$:\n",
    "If we observed data given by $y_i$, such that the fitted predicts $f_i$ for each point i, we can write the mean of all the observed data, given by $y_{mean}$ as:\n",
    "$$y_{mean} = \\frac{1}{n}\\sum_{i}{y_i}$$\n",
    "\n",
    "* Total sum of sqaures, which is proportional to the variance of the data is\n",
    "$$ SS_{total} = \\sum_{i} (y_i - y_{mean})^2$$\n",
    "\n",
    "* The residual sum of squares (also called the error) is defined as:\n",
    "$$ SS_{res} = \\sum_{i} (y_i - f_{i})^2$$\n",
    "\n",
    "* Note the $R^2$ is defined as:\n",
    "$$ R^2 = 1 - \\frac {SS_{res}}{SS_{total}}$$\n",
    "\n",
    "### How to intepret $R^2$:\n",
    "* $R^2$ is 1  for a model that perfectly fits with the observed data\n",
    "* If the model predicts $y_{mean}$ always then $SS_{res} = SS_{total}$ and $R^2 = 0$, this indicates a baseline model to which all other models can be compared.\n",
    "* Any model that performs worse than the baseline model whill have a negative $R^2$ score.\n",
    "\n",
    "### Explained variance\n",
    "The term $\\frac {SS_{res}}{SS_{total}}$ is also called Unexplained Variance.\n",
    "\n",
    "### PPC\n",
    "\n",
    "#### MSE\n",
    "used to measure the error in our model with regards to the data that the model is trying to fit.\n",
    "Mean sqaured error (MSE) is the most intuitive index:\n",
    "\n",
    "$$MSE=\\frac {\\sum_{1}^{n}(y_{true} - y_{predicted})^2} {n}$$\n",
    "\n",
    "\n",
    "## Cross validation\n",
    "In k-fold cross-validation, we divide the data into k-folds or subsets, and perform training of the model on k minus 1 folds or the model performance is assessed on the one fold that's left. \n",
    "\n",
    "If the number of folds is equal to the number of data points, we have leave-one-out cross validation. \n",
    "\n",
    "The ideal way to test is using another independent sample, 'out-of-sample' prediction.\n",
    "\n",
    "### loo-PSIS\n",
    "\n",
    "\n",
    "## Information criteria\n",
    "1. Log-likelihood (Log predictive density) and deviance\n",
    "2. Akaike information criterion (AIC)\n",
    "3. Widely applicable information criterion (WAIC)\n",
    "4. Deviance information criterion (DIC)\n",
    "5. Bayesian Information criterion (BIC)\n",
    "\n",
    "From (2) to (5), \n",
    "* They take the form of the equation with two terms given by\n",
    "$$metric = model fit + penalization$$\n",
    "* The model fit is measured using log likelihood of the data given model parameters (could be a pointwise estimate, or could use the full posterior distribution)\n",
    "* Lower values imply a better fit\n",
    "\n",
    "AIC, BIC, and DIC use the joint probability of the data, where waic computes the pointwise probability of the data.\n",
    "\n",
    "**Note**: AIC and BIC are regarded as not Bayesian criteria and was excluded in statistical rethinking (2ed).\n",
    "\n",
    "### Log-likelihood and Deviance\n",
    "These terms, like MSE, are used to measure the error in our model with regards to the data that the model is trying to fit.\n",
    "\n",
    "MSE is acceptable especially when the likelihood is a normal distribution. \n",
    "\n",
    "#### Log-likelihood\n",
    "\n",
    "---\n",
    "\n",
    "over-thinking: likelihood\n",
    "\n",
    "Likelihood is the probability that a population with a specified set of parameters was examined, given a sample of observations.\n",
    "\n",
    "$$L(a population | x_1, x_2, x_3, ..., x_n)$$\n",
    "\n",
    "$$L(a population | x) = P(x | a population)$$\n",
    "\n",
    "--- \n",
    "\n",
    "A more theoretically justified way to measure the performance of a model is using the log-likelihood function.\n",
    "\n",
    "$$Log likelihood = \\sum_{1}^{n} logp(y_i | \\theta)$$\n",
    "\n",
    "If the likelihood function is a Normal, the log-likelihood is proportional to the MSE\n",
    "\n",
    "#### Deviance\n",
    "\n",
    "$$ Deviance = -2 \\sum_{1}^{n} (logp(y_i | \\theta) - logp(y_i | \\theta_s)$$\n",
    "\n",
    "where $logp(y_i | \\theta_s)$ is the likelihood of saturated model, and $\\theta_s$ is the parameters of a saturated model, which is a model that overfitted to the point that it fits the observed data perfectly, range form 0 to $\\infty$\n",
    "\n",
    "$p(y_i | \\theta)$ range form 0 to 1, 0 means not fit at all, and 1 means perfect fit. $\\sum_{1}^{n} logp(y_i | \\theta)$ take the log of likelihood, thus takes value from $-\\infty$ to $0$. Mutiplying the log-likelihood function by $-2$ results in a number that is interpreatable similar to the MSE:\n",
    "* Poorly fit models have large positive values\n",
    "* A perfectly fit model has a value of 0\n",
    "\n",
    "***over-thinking: MLE and deviance:***\n",
    "Maximum likelihood Estimation (MLE): estimating the parameters $\\theta$ that maximizing the probability \\sum_1^n p(y_i|\\theta). MLE is the most efficient estimator for the distribution parameter $\\theta$. A disadvantage of the MLE when you have non-regular distributions, i.e., distribution whose parameters are constrained by the observed values. For such distributions, a maximum likelihood may not exist.\n",
    "\n",
    "**Question**: How the saturated model was obtained?\n",
    "\n",
    "### Posterior Predictive Distribution\n",
    "\n",
    "$$ accuracy = p(y_{new} | y) =  \\int p(y_{new} | y) p(\\theta|y) \\, d\\theta $$\n",
    "where $p(\\theta|y)$ is the posterior distribution for $\\theta$ and we integrate over the entire distribution of $\\theta$. it is the average of all the probabilities of seeing $y_{new}$ calculated over all possible values of $\\theta$.\n",
    "\n",
    "$$ accuracy = E[p(y_{new} | \\theta)]$$\n",
    "\n",
    "This has the following steps:\n",
    "1. Draw a $\\theta_{i}$ from the posterior distribution for $\\theta$\n",
    "2. Given the value of $\\theta_{i}$, how likely are you to see $y_{new}$ or compute $p(y_{new} | \\theta)$.\n",
    "3. Repeat the above two steps several times to compute the expectation of $p(y_{new} | \\theta)$.\n",
    "\n",
    "This index is also computed using log as:\n",
    "$$ accuracy = log(E[p(y_{new} | \\theta)])$$\n",
    "\n",
    "\n",
    "### AIC (Akaike Information Criterion)\n",
    "\n",
    "$$ AIC = -2 \\sum_{i=1}^{n} logp(y_i | \\theta_{mle}) + 2n_{parameter}$$, where $n_{parameter}$ refers to the number of parameters in the model and $\\theta_{mle}$ is the MLE estimate of $\\theta$. \n",
    "\n",
    "AIC does not use the the posterior distribution, so it does not take into account any information regarding the uncertainty of the parameters. \n",
    "\n",
    "### BIC (Bayesian Information Criterion)\n",
    "\n",
    "$$ BIC = -2 \\sum_{i=1}^{n} logp(y_i | \\theta_{mle}) + n_{parameter} log(n_{sample})$$, \n",
    "\n",
    "\n",
    "### DIC (Deviance Information Criterion)\n",
    "\n",
    "$$ DIC = -2 \\sum_{i=1}^{n} logp(y_i | \\theta_{Bayes}) + 2 var_{posterior} logp(y_i | \\theta)$$, \n",
    "\n",
    "**what does the $\\theta_{Bayes}$ and the $\\theta$ mean here??**\n",
    "\n",
    "\n",
    "### WAIC (Widely applicable information criterion)\n",
    "The deviation for the log pointwise predictive density is log likelihood.\n",
    "\n",
    "#### Log pointwise predictive density (lppd)\n",
    "$$p_{post}(y_{new}) = \\int p(y_{new} | \\theta) p_{post}(\\theta) d\\theta$$\n",
    "\n",
    "if we take the log of both sides:\n",
    "\n",
    "$$log(p_{post}(y_{new})) = log(\\int p(y_{new} | \\theta) p_{post}(\\theta) d\\theta)$$, where $p_{post}\\theta$ is the posterior distribution of $\\theta$ obstained by training set. This is predictive fit of the new point. if we have a number of new data point $i = 1, 2, .., n$, we can write the following for the log pointwise predicitve density for a model using the new data:\n",
    "\n",
    "$$lppd = log \\prod_{i} p_{post}(y_{new_i}) = \\sum_{i} \\int logp(y_{new} | \\theta) p_{post}(\\theta) d\\theta $$\n",
    "\n",
    "* in practice, the inner integral over $\\theta$ is computed using an average over possible values of $\\theta$ (sampled) denoted as $\\theta_{S}$.\n",
    "\n",
    "$$\\sum_{i} \\int logp(y_{new} | \\theta) p_{post}(\\theta) d\\theta = \\sum_{i} log \\frac {1} {S} \\sum_{S}p(y_{new_{i}} | \\theta_{S})$$\n",
    "\n",
    "* Now suppose we donot have new dataset $y_{new}$ and we compute the $lppd$ over our training set, that is not a good measure for future performance of the model. So WAIC adds a term to correct for this overestimated performance.\n",
    "\n",
    "$$2*\\sum_{i} Var_{s}(logp(y_{new} | \\theta_{S}))$$\n",
    "\n",
    "* so WAIC is defined as the sum of the two items above:\n",
    "\n",
    "$$WAIC = -2 \\sum_{i} log \\frac {1} {S} \\sum_{S}p(y_{new_{i}} | \\theta_{S}) + 2*\\sum_{i} Var_{s}(logp(y_{new} | \\theta_{S}))$$\n",
    "\n",
    "## Hierarchical models\n",
    "\n",
    "In `PyMC3`, for hierarchical model, log likelihood has only **four** dimensions: chain, draw, obs_dim_0, obs_dim_1.\n",
    "\n",
    "As Oriol Abril mentioned in [here](https://discourse.pymc.io/t/modeling-human-response-time-data-with-an-exponential-model-model-comparison-issues/3666/9) and [here](https://discourse.pymc.io/t/calculating-waic-for-models-with-multiple-likelihood-functions/4834/5), hierarchical models has different ways for comparing `loo` and `waic`.\n",
    "\n",
    "Some useful resources for understanding this: https://avehtari.github.io/modelselection/rats_kcv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the log-likelihood of HDDM\n",
    "\n",
    "#### First, figure out how the `DIC` is calculated\n",
    "\n",
    "##### `DIC`'s formula as describe in Wieckie et al (2013) ?\n",
    "\n",
    "Actually, there is no formula for DIC in that paper, only a reference to Speigelhalter et a., 2019 was give.\n",
    "\n",
    "##### `DIC`'s common formula (Student's Guide)\n",
    "\n",
    "$$ DIC = 2 * \\widehat {elpd_{DIC}} $$\n",
    "$$\\widehat {elpd_{DIC}} = log[(y | \\hat{\\theta}_{Bayes})] - k_{DIC}$$\n",
    "$$ k_{DIC} = 2var_{posterior}(log[p(y | \\theta)]$$\n",
    "\n",
    "\n",
    "##### How `DIC` is calculated in the HDDM script:\n",
    "\n",
    "In `HDDM` the `DIC` information is calcuated by referring the `dic_info()` function, which is define @L665 in the kabuki/hierarchical.py. In this piece of code, it used the `DIC` function from `MCMC` object of `pymc2`. \n",
    "\n",
    "In the `pymc2`'s script (pymc/MCMC.py), we can find `_calc_dic` @L419, where the `DIC` is calculated from two variables: `mean_deviance` and `self.deviance`.\n",
    "\n",
    "`self.deviance` is a property of `MCMC` model object, which was defined as `-2 * sum[v.get_logp() for v in self.observed_stochastics])` [@L219 of the pymc/Model.py](https://github.com/pymc-devs/pymc/blob/6b1b51ddea1a74c50d9a027741252b30810b29e0/pymc/Model.py#L219).\n",
    "\n",
    "`mean_deviance = np.mean(self.db.trace('deviance')(). axis=0)`\n",
    "\n",
    "`dic` is `2* mean_deviance - self.deviance`\n",
    "\n",
    "\n",
    "**Here is the unsolved issues:**\n",
    "\n",
    "* `mean_deviance` is the mean from `tmp.mc.db.trace('deviance')()`, if the `deviance` in the trace is defined as in @L842 in `pymc2/Model.py`, then, it uses `_sum_deviance()`, which used the `-2*sum([v.get_logp() for v in self.observed_stochastic])`. However, `self.observed_stochastic` has 42 elements, each is a `wtfp` distribution object, `tmp.mc.db.trace('deviance')` has 1500 elements, which is the same as MCMC sample. So, it seems to me that `tmp.mc.db.trace('deviance')` and the loglikelihood calculated from `self.observed_stochastic` is differnt.\n",
    "\n",
    "* `logp` is determined by `wftp_like`?\n",
    "\n",
    "\n",
    "arviz/stats/stats_utils.py#L257\n",
    "the loglikelihoohd was stored in sample_stats groups,but has been deprecated.\n",
    "\n",
    "#### Second, compare `DIC` and `WAIC`\n",
    "\n",
    "##### `WAIC` in Student's guide:\n",
    "\n",
    "We consider each of the $n$ data points separately. Consider a single data point $y_i$, we can take the log of the average value of the likelihood across the posterior distribution:\n",
    "\n",
    "$\\widehat{lpd} = log[E_{posterior} (p(y_i | \\theta))]$\n",
    "\n",
    "Where $E_{posterior}$ is the expectation with respect to the posterior distribution. If we sum corresponding terms for each of the $n$ points, and include a bias correction term, we obtain an estimate of the expected log pointwise predicitive density (*elppd*):\n",
    "\n",
    "$\\widehat{elppd} = \\sum_{i=1}^{n} log[E_{postereior}(p(y_i | \\theta))] - k_{WAIC}$\n",
    "\n",
    "where:\n",
    "\n",
    "$k_{WAIC} = \\sum_{i=1}^{n} var_{posterior}(log p(y_i | \\theta))$.\n",
    "\n",
    "$WAIC = -2 * \\widehat{elppd}$\n",
    "\n",
    "\n",
    "`log_likelihood` in `ArviZ` is the pointwise loglikelihood, where the `Samples` should match with `posterior` ones (1500 in this case) and its variable should match `observed_data` variable (42 = 14 * 3) in this case. which means, for each sample (totally 1500), we can calculate the likelihood for each data point (14 * 3 * 1500 arrays, the length of each array depends on the number of data points there).\n",
    "\n",
    "Differences between `logp` in `DIC` and `point-wise log-likelihood` (which uses `pdf` function) in `WAIC` or `loo`:\n",
    "\n",
    "In the student's guide to Bayesian statistics, point-wise likelihood is the log-likelihood for each data point across all posterior samples (page 393).\n",
    "\n",
    "```\n",
    "for (i in 1:N){\n",
    "    loglikelihood[i] = normal_lpdf(X[i] | mu, sigma);\n",
    "    }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAIC and LOO for Hierachical Models\n",
    "\n",
    "https://discourse.pymc.io/t/calculating-waic-for-models-with-multiple-likelihood-functions/4834/19\n",
    "https://discourse.pymc.io/t/modeling-human-response-time-data-with-an-exponential-model-model-comparison-issues/3666/9\n",
    "\n",
    "https://avehtari.github.io/modelselection/rats_kcv.html\n",
    "\n",
    "https://statmodeling.stat.columbia.edu/2018/08/03/loo-cross-validation-approaches-valid/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
