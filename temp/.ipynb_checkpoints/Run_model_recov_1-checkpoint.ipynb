{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is running: /opt/conda/bin/python\n",
      "The current Python version is 3.7.6\n",
      "The current HDDM version is 0.8.0\n",
      "The current Kabuki version is 0.6.3\n",
      "The current PyMC version is 2.3.8\n",
      "The current IPython version is 7.15.0\n",
      "The current Numpy version is 1.19.4\n",
      "The current Pandas version is 1.0.5\n",
      "The current seaborn version is 0.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated since IPython 4.0. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('Notebook is running:', sys.executable)\n",
    "\n",
    "# further check your python version\n",
    "from platform import python_version\n",
    "\n",
    "print('The current Python version is', python_version())\n",
    "\n",
    "# If you are sure that conda is installed, also check the package that install\n",
    "#!conda list  # list the conda\n",
    "\n",
    "import hddm, IPython, kabuki, pymc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "print('The current HDDM version is', hddm.__version__) # 0.8.0\n",
    "print('The current Kabuki version is', kabuki.__version__) # 0.6.3\n",
    "print('The current PyMC version is', pymc.__version__) # 2.3.8\n",
    "\n",
    "# Warning:`IPython.parallel` package has been deprecated since IPython 4.0. \n",
    "print('The current IPython version is', IPython.__version__) \n",
    "\n",
    "print('The current Numpy version is', np.__version__) \n",
    "\n",
    "print('The current Pandas version is', pd.__version__)\n",
    "\n",
    "print('The current seaborn version is', sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import os, hddm, time, csv\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "import pymc as pm\n",
    "import hddm\n",
    "import kabuki\n",
    "\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from patsy import dmatrix\n",
    "\n",
    "from p_tqdm import p_map\n",
    "from functools import partial\n",
    "\n",
    "# set the color of plots\n",
    "from cycler import cycler\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color='bgrcmykw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: I hacked the `post_pred_gen`, \n",
    "# more detals: https://groups.google.com/g/hddm-users/c/Is6AM7eN0fo\n",
    "from post_pred_gen_redifined import _parents_to_random_posterior_sample\n",
    "from post_pred_gen_redifined import _post_pred_generate\n",
    "from post_pred_gen_redifined import post_pred_gen\n",
    "\n",
    "from pointwise_loglik_gen import _pointwise_like_generate\n",
    "from pointwise_loglik_gen import pointwise_like_gen\n",
    "\n",
    "# import self-defined functions\n",
    "from SimData import SimData\n",
    "from run_models import run_m1, run_m2, run_m4, run_m5, run_m7\n",
    "\n",
    "model_func = [run_m1, run_m2, run_m4, run_m5, run_m7]\n",
    "\n",
    "m_keys = [\"ms1\",\n",
    "          \"ms2\",\n",
    "          \"ms4\",\n",
    "          \"ms5\",\n",
    "          \"ms7\"]\n",
    "\n",
    "df_keys = [\"sim_df1\", \n",
    "           \"sim_df2\", \n",
    "           \"sim_df4\", \n",
    "           \"sim_df5\",\n",
    "           \"sim_df7\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_recov(data=None, m_keys=None, model_func=None):\n",
    "    \"\"\"\n",
    "    This func is for model recovery. \n",
    "    \n",
    "    data: input data, can be simulated data or real data\n",
    "    m_keys: id for different models\n",
    "    model_func: a list of model functions\n",
    "    \n",
    "    \"\"\"\n",
    "    InfData = {}\n",
    "    models = {}\n",
    "    for ii in range(len(m_keys)):\n",
    "        m_key = m_keys[ii]\n",
    "\n",
    "        ### Run models\n",
    "        save_name = \"./tmp/\" + m_key + \"_tmp\"\n",
    "        print(\"start model fitting for \", m_key)\n",
    "        ms_tmp = p_map(partial(model_func[ii], \n",
    "                               df=data, \n",
    "                               samples=samples,\n",
    "                               burn=burn,\n",
    "                               save_name=save_name),\n",
    "                       range(chains))\n",
    "\n",
    "        ### Observations\n",
    "        xdata_observed = ms_tmp[0].data.copy()\n",
    "        xdata_observed.index.names = ['trial_idx']\n",
    "        xdata_observed = xdata_observed[['rt', 'response']]\n",
    "        xdata_observed = xr.Dataset.from_dataframe(xdata_observed)\n",
    "\n",
    "        ### posteriors\n",
    "        xdata_posterior = []\n",
    "        for jj in range(len(ms_tmp)):\n",
    "            trace_tmp = ms_tmp[jj].get_traces()\n",
    "            trace_tmp['chain'] = jj\n",
    "            trace_tmp['draw'] = np.arange(len(trace_tmp), dtype=int)\n",
    "            xdata_posterior.append(trace_tmp)\n",
    "        xdata_posterior = pd.concat(xdata_posterior)\n",
    "        xdata_posterior = xdata_posterior.set_index([\"chain\", \"draw\"])\n",
    "        xdata_posterior = xr.Dataset.from_dataframe(xdata_posterior)\n",
    "\n",
    "        ### PPC\n",
    "        xdata_post_pred = [] # define an empty dict    \n",
    "        print(\"start PPC for \", m_key)\n",
    "        start_time = time.time()  \n",
    "        xdata_post_pred = p_map(partial(post_pred_gen), ms_tmp)\n",
    "        print(\"Running PPC for \", m_key, \" costs %f seconds\" % (time.time() - start_time))\n",
    "        xdata_post_pred = pd.concat(xdata_post_pred, names=['chain'], \n",
    "                                keys = list(range(len(xdata_post_pred))))\n",
    "        xdata_post_pred = xdata_post_pred.reset_index(level=1, drop=True)\n",
    "        xdata_post_pred = xr.Dataset.from_dataframe(xdata_post_pred)\n",
    "\n",
    "        ### Point-wise log likelihood\n",
    "        xdata_loglik = [] # define an empty dict\n",
    "        print(\"start calculating loglik for \", m_key)\n",
    "        start_time = time.time()  # the start time of the processing\n",
    "        xdata_loglik = p_map(partial(pointwise_like_gen), ms_tmp)\n",
    "        print(\"Generating loglik costs %f seconds\" % (time.time() - start_time))\n",
    "\n",
    "        xdata_loglik = pd.concat(xdata_loglik, names=['chain'], \n",
    "                                keys = list(range(len(xdata_loglik))))\n",
    "        xdata_loglik = xdata_loglik.reset_index(level=1, drop=True)\n",
    "        xdata_loglik = xr.Dataset.from_dataframe(xdata_loglik)\n",
    "        \n",
    "        ### convert to InfData\n",
    "        InfData[m_key] = az.InferenceData(posterior=xdata_posterior, \n",
    "                                                 observed_data=xdata_observed,\n",
    "                                                 posterior_predictive=xdata_post_pred,\n",
    "                                                 log_likelihood = xdata_loglik)\n",
    "        models[m_key] = ms_tmp\n",
    "    return models, InfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 2000\n",
    "burn = 500\n",
    "chains = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model recovery for  sim_df1\n",
      "start model fitting for  ms1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5171f497d464689b68e73bedba3ca45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/optimize.py:2149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/optimize.py:2149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/optimize.py:2149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/optimize.py:2149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [                  0%                  ] 3 of 2000 complete in 0.7 sec[                  0%                  ] 3 of 2000 complete in 0.6 sec[                  0%                  ] 3 of 2000 complete in 0.8 sec[                  0%                  ] 4 of 2000 complete in 0.9 sec[                  0%                  ] 4 of 2000 complete in 1.3 sec[                  0%                  ] 4 of 2000 complete in 1.3 sec[                  0%                  ] 4 of 2000 complete in 1.4 sec[                  0%                  ] 5 of 2000 complete in 1.4 sec"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "conf_mat_dic1 = pd.DataFrame(0, index=m_keys, columns=df_keys)\n",
    "conf_mat_loo1 = pd.DataFrame(0, index=m_keys, columns=df_keys)\n",
    "conf_mat_waic1 = pd.DataFrame(0, index=m_keys, columns=df_keys)\n",
    "\n",
    "for sim in range (4):   \n",
    "    for df_key in df_keys:\n",
    "        ### simulate data\n",
    "        data = SimData(df_key)\n",
    "\n",
    "        ### fit the sim data\n",
    "        print(\"Start model recovery for \", df_key)\n",
    "        models, InfData = model_recov(data=data, m_keys=m_keys, model_func=model_func)\n",
    "\n",
    "        ### compare models\n",
    "        tmp_loo_comp = az.compare(InfData, ic=\"loo\")\n",
    "        tmp_loo_comp = tmp_loo_comp.reset_index()\n",
    "        tmp_waic_comp = az.compare(InfData, ic=\"waic\")\n",
    "        tmp_waic_comp = tmp_waic_comp.reset_index()\n",
    "        \n",
    "        tmp_dic = []\n",
    "        indx_name = []\n",
    "\n",
    "        for m_key, model in models.items():\n",
    "            m_tmp = kabuki.utils.concat_models(model)\n",
    "            tmp_dic.append(m_tmp.dic)\n",
    "            indx_name.append(m_key)\n",
    "            \n",
    "        tmp_dic_comp = pd.DataFrame(tmp_dic, index=indx_name, columns=['dic'])\n",
    "        tmp_dic_comp = tmp_dic_comp.sort_values(by=['dic'])\n",
    "        tmp_dic_comp = tmp_dic_comp.reset_index()\n",
    "        #conf_mat_dic.rename(columns={'index':'rank'}, inplace=True)\n",
    "\n",
    "        ### record the best models\n",
    "        conf_mat_dic1.loc[tmp_dic_comp.loc[0, 'index'], df_key] += 1\n",
    "        conf_mat_loo1.loc[tmp_loo_comp.loc[0, 'index'], df_key] += 1\n",
    "        conf_mat_waic1.loc[tmp_waic_comp.loc[0, 'index'], df_key] += 1\n",
    "\n",
    "        conf_mat_dic1.to_csv('conf_mat_dic1.csv')\n",
    "        conf_mat_loo1.to_csv('conf_mat_loo1.csv')\n",
    "        conf_mat_waic1.to_csv('conf_mat_waic1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_loo_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_waic_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['ms7'][0].plot_posteriors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
