{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is running: /opt/conda/bin/python\n",
      "The current HDDM version is 3.7.6\n",
      "The current HDDM version is 0.8.0\n",
      "The current Kabuki version is 0.6.3\n",
      "The current PyMC version is 2.3.8\n",
      "The current IPython version is 7.15.0\n",
      "The current Numpy version is 1.19.4\n",
      "The current Pandas version is 1.0.5\n",
      "The current seaborn version is 0.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated since IPython 4.0. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('Notebook is running:', sys.executable)\n",
    "\n",
    "# further check your python version\n",
    "from platform import python_version\n",
    "\n",
    "print('The current HDDM version is', python_version())\n",
    "\n",
    "# If you are sure that conda is installed, also check the package that install\n",
    "#!conda list  # list the conda\n",
    "\n",
    "import hddm, IPython, kabuki, pymc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "print('The current HDDM version is', hddm.__version__) # 0.8.0\n",
    "print('The current Kabuki version is', kabuki.__version__) # 0.6.3\n",
    "print('The current PyMC version is', pymc.__version__) # 2.3.8\n",
    "\n",
    "# Warning:`IPython.parallel` package has been deprecated since IPython 4.0. \n",
    "print('The current IPython version is', IPython.__version__) \n",
    "\n",
    "print('The current Numpy version is', np.__version__) \n",
    "\n",
    "print('The current Pandas version is', pd.__version__)\n",
    "\n",
    "print('The current seaborn version is', sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import os, hddm, time, csv\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "import pymc as pm\n",
    "import hddm\n",
    "import kabuki\n",
    "\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from patsy import dmatrix\n",
    "\n",
    "from p_tqdm import p_map\n",
    "from functools import partial\n",
    "\n",
    "# set the color of plots\n",
    "from cycler import cycler\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color='bgrcmykw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_tqdm import p_map\n",
    "from functools import partial\n",
    "\n",
    "# NOTE: I hacked the `post_pred_gen`, \n",
    "# more detals: https://groups.google.com/g/hddm-users/c/Is6AM7eN0fo\n",
    "from post_pred_gen_redifined import _parents_to_random_posterior_sample\n",
    "from post_pred_gen_redifined import _post_pred_generate\n",
    "from post_pred_gen_redifined import post_pred_gen\n",
    "\n",
    "from pointwise_loglik_gen import _pointwise_like_generate\n",
    "from pointwise_loglik_gen import pointwise_like_gen\n",
    "\n",
    "from SimData import SimData\n",
    "from run_models import run_m1, run_m2, run_m4, run_m5, run_m7\n",
    "\n",
    "model_func = [run_m1, run_m2, run_m4, run_m5, run_m7]\n",
    "\n",
    "m_keys = [\"ms1\",\n",
    "          \"ms2\",\n",
    "          \"ms4\",\n",
    "          \"ms5\",\n",
    "          \"ms7\"]\n",
    "\n",
    "df_keys = [\"sim_df1\", \n",
    "           \"sim_df2\", \n",
    "           \"sim_df4\", \n",
    "           \"sim_df5\",\n",
    "           \"sim_df7\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_recov(m_keys, data, model_func):\n",
    "    \"\"\"\n",
    "    This func is for model recovery\n",
    "    \n",
    "    input: \n",
    "    m_keys\n",
    "    df_keys\n",
    "    \n",
    "    \"\"\"\n",
    "    InfData = {}\n",
    "    for ii in range(len(m_keys)):\n",
    "        m_key = m_keys[ii]\n",
    "\n",
    "        ### Run models\n",
    "        ms_tmp = p_map(partial(model_func[ii], \n",
    "                               df=data, \n",
    "                               samples=samples,\n",
    "                               burn=burn,\n",
    "                               save_name=\"model_tmp\"),\n",
    "                       range(chains))\n",
    "\n",
    "        ### Observations\n",
    "        xdata_observed = ms_tmp[0].data.copy()\n",
    "        xdata_observed.index.names = ['trial_idx']\n",
    "        xdata_observed = xdata_observed[['rt', 'response']]\n",
    "        xdata_observed = xr.Dataset.from_dataframe(xdata_observed)\n",
    "\n",
    "        ### posteriors\n",
    "        xdata_posterior = []\n",
    "        for jj in range(len(ms_tmp)):\n",
    "            trace_tmp = ms_tmp[jj].get_traces()\n",
    "            trace_tmp['chain'] = jj\n",
    "            trace_tmp['draw'] = np.arange(len(trace_tmp), dtype=int)\n",
    "            xdata_posterior.append(trace_tmp)\n",
    "        xdata_posterior = pd.concat(xdata_posterior)\n",
    "        xdata_posterior = xdata_posterior.set_index([\"chain\", \"draw\"])\n",
    "        xdata_posterior = xr.Dataset.from_dataframe(xdata_posterior)\n",
    "\n",
    "        ### PPC\n",
    "        xdata_post_pred = [] # define an empty dict        \n",
    "        start_time = time.time()  \n",
    "        xdata_post_pred = p_map(partial(post_pred_gen), ms_tmp)\n",
    "        print(\"Running PPC costs %f seconds\" % (time.time() - start_time))\n",
    "        xdata_post_pred = pd.concat(xdata_post_pred, names=['chain'], \n",
    "                                keys = list(range(len(xdata_post_pred))))\n",
    "        xdata_post_pred = xdata_post_pred.reset_index(level=1, drop=True)\n",
    "        xdata_post_pred = xr.Dataset.from_dataframe(xdata_post_pred)\n",
    "\n",
    "        ### Point-wise log likelihood\n",
    "        xdata_loglik = [] # define an empty dict\n",
    "        start_time = time.time()  # the start time of the processing\n",
    "        xdata_loglik = p_map(partial(pointwise_like_gen), ms_tmp)\n",
    "        print(\"Generating loglik costs %f seconds\" % (time.time() - start_time))\n",
    "\n",
    "        xdata_loglik = pd.concat(xdata_loglik, names=['chain'], \n",
    "                                keys = list(range(len(xdata_loglik))))\n",
    "        xdata_loglik = xdata_loglik.reset_index(level=1, drop=True)\n",
    "        xdata_loglik = xr.Dataset.from_dataframe(xdata_loglik)\n",
    "\n",
    "        InfData[m_key] = az.InferenceData(posterior=xdata_posterior, \n",
    "                                                 observed_data=xdata_observed,\n",
    "                                                 posterior_predictive=xdata_post_pred,\n",
    "                                                 log_likelihood = xdata_loglik)\n",
    "    return ms_tmp, InfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1000\n",
    "burn = 500\n",
    "chains = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79ab11264f5484ba8754f3a5c3cdcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/optimize.py:2149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/optimize.py:2149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/optimize.py:2149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/optimize.py:2149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [                  0%                  ] 3 of 1000 complete in 0.8 sec[                  0%                  ] 3 of 1000 complete in 0.8 sec[                  0%                  ] 3 of 1000 complete in 1.0 sec[                  0%                  ] 3 of 1000 complete in 1.0 sec[                  0%                  ] 4 of 1000 complete in 1.4 sec[                  0%                  ] 4 of 1000 complete in 1.5 sec[                  0%                  ] 4 of 1000 complete in 1.6 sec[                  0%                  ] 4 of 1000 complete in 1.6 sec[                  0%                  ] 5 of 1000 complete in 1.9 sec[                  0%                  ] 5 of 1000 complete in 2.1 sec[                  0%                  ] 5 of 1000 complete in 2.1 sec[                  0%                  ] 5 of 1000 complete in 2.2 sec[                  0%                  ] 7 of 1000 complete in 2.7 sec[                  0%                  ] 6 of 1000 complete in 2.6 sec[                  0%                  ] 6 of 1000 complete in 2.7 sec[                  0%                  ] 6 of 1000 complete in 2.7 sec[                  0%                  ] 7 of 1000 complete in 3.2 sec[                  0%                  ] 7 of 1000 complete in 3.3 sec[                  0%                  ] 7 of 1000 complete in 3.4 sec[                  0%                  ] 9 of 1000 complete in 3.7 sec[                  0%                  ] 8 of 1000 complete in 3.8 sec[                  0%                  ] 8 of 1000 complete in 3.8 sec[                  0%                  ] 9 of 1000 complete in 4.3 sec[                  0%                  ] 9 of 1000 complete in 4.3 sec[                  0%                  ] 9 of 1000 complete in 4.4 sec[                  1%                  ] 11 of 1000 complete in 4.8 sec[                  1%                  ] 10 of 1000 complete in 4.8 sec[                  1%                  ] 10 of 1000 complete in 4.9 sec[                  1%                  ] 10 of 1000 complete in 4.9 sec[                  1%                  ] 12 of 1000 complete in 5.3 sec[                  1%                  ] 11 of 1000 complete in 5.4 sec[                  1%                  ] 11 of 1000 complete in 5.5 sec[                  1%                  ] 13 of 1000 complete in 5.8 sec[                  1%                  ] 12 of 1000 complete in 5.9 sec[                  1%                  ] 12 of 1000 complete in 6.1 sec[                  1%                  ] 14 of 1000 complete in 6.5 sec[                  1%                  ] 13 of 1000 complete in 6.5 sec[                  1%                  ] 13 of 1000 complete in 6.5 sec[                  1%                  ] 15 of 1000 complete in 7.0 sec[                  1%                  ] 14 of 1000 complete in 7.0 sec[                  1%                  ] 14 of 1000 complete in 7.1 sec[                  1%                  ] 14 of 1000 complete in 7.1 sec[                  1%                  ] 15 of 1000 complete in 7.6 sec[                  1%                  ] 15 of 1000 complete in 7.7 sec[                  1%                  ] 15 of 1000 complete in 7.7 sec[                  1%                  ] 17 of 1000 complete in 8.1 sec[                  1%                  ] 16 of 1000 complete in 8.1 sec[                  1%                  ] 16 of 1000 complete in 8.2 sec[                  1%                  ] 18 of 1000 complete in 8.7 sec[                  1%                  ] 17 of 1000 complete in 8.7 sec[                  1%                  ] 17 of 1000 complete in 8.7 sec[                  1%                  ] 17 of 1000 complete in 8.7 sec[                  1%                  ] 19 of 1000 complete in 9.2 sec[                  1%                  ] 18 of 1000 complete in 9.3 sec[                  2%                  ] 20 of 1000 complete in 9.8 sec[                  1%                  ] 19 of 1000 complete in 9.8 sec[                  1%                  ] 19 of 1000 complete in 9.8 sec[                  1%                  ] 19 of 1000 complete in 9.7 sec[                  2%                  ] 20 of 1000 complete in 10.3 sec[                  2%                  ] 21 of 1000 complete in 10.4 sec[                  2%                  ] 20 of 1000 complete in 10.4 sec[                  2%                  ] 21 of 1000 complete in 10.8 sec[                  2%                  ] 21 of 1000 complete in 10.9 sec[                  2%                  ] 21 of 1000 complete in 11.0 sec[                  2%                  ] 22 of 1000 complete in 11.1 sec[                  2%                  ] 22 of 1000 complete in 11.5 sec[                  2%                  ] 23 of 1000 complete in 11.7 sec[                  2%                  ] 23 of 1000 complete in 11.8 sec[                  2%                  ] 23 of 1000 complete in 11.9 sec[                  2%                  ] 23 of 1000 complete in 12.1 sec[                  2%                  ] 24 of 1000 complete in 12.3 sec[                  2%                  ] 24 of 1000 complete in 12.4 sec[                  2%                  ] 24 of 1000 complete in 12.7 sec[                  2%                  ] 25 of 1000 complete in 12.9 sec[                  2%                  ] 25 of 1000 complete in 13.0 sec[                  2%                  ] 25 of 1000 complete in 13.0 sec[                  2%                  ] 25 of 1000 complete in 13.3 sec[                  2%                  ] 26 of 1000 complete in 13.4 sec[                  2%                  ] 26 of 1000 complete in 13.5 sec[                  2%                  ] 26 of 1000 complete in 13.6 sec[                  2%                  ] 26 of 1000 complete in 13.8 sec[-                 2%                  ] 27 of 1000 complete in 14.0 sec[-                 2%                  ] 27 of 1000 complete in 14.1 sec[-                 2%                  ] 27 of 1000 complete in 14.2 sec[-                 2%                  ] 27 of 1000 complete in 14.4 sec[-                 2%                  ] 28 of 1000 complete in 14.6 sec[-                 2%                  ] 28 of 1000 complete in 14.6 sec[-                 2%                  ] 28 of 1000 complete in 14.8 sec[-                 2%                  ] 28 of 1000 complete in 14.9 sec[-                 2%                  ] 29 of 1000 complete in 15.2 secHalting at iteration "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-14:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-15:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halting at iteration Halting at iteration "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  272827"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "conf_mat_dic0 = pd.DataFrame(0, index=m_keys, columns=df_keys)\n",
    "conf_mat_loo0 = pd.DataFrame(0, index=m_keys, columns=df_keys)\n",
    "conf_mat_waic0 = pd.DataFrame(0, index=m_keys, columns=df_keys)\n",
    "\n",
    "for sim in range (5):   \n",
    "    for df_key in df_keys:\n",
    "        ### simulate data\n",
    "        data = SimData(df_key)\n",
    "\n",
    "        ### fit the sim data\n",
    "        models, InfData = model_recov(m_keys, data, model_func)\n",
    "\n",
    "        ### compare models\n",
    "        tmp_loo_comp = az.compare(InfData[df_key], ic=\"loo\")\n",
    "        tmp_loo_comp = tmp_loo_comp.reset_index()\n",
    "        tmp_waic_comp = az.compare(InfData[df_key], ic=\"waic\")\n",
    "        tmp_waic_comp = tmp_loo_comp.reset_index()\n",
    "        \n",
    "        tmp_dic = []\n",
    "        indx_name = []\n",
    "\n",
    "        for m_key, model in models[df_key].items():\n",
    "            m_tmp = kabuki.utils.concat_models(model)\n",
    "            tmp_dic.append(m_tmp.dic)\n",
    "            indx_name.append(m_key)\n",
    "            \n",
    "        conf_mat_dic = pd.DataFrame(tmp_dic, index=indx_name, columns=['dic'])\n",
    "        conf_mat_dic = conf_mat_dic.sort_values(by=['dic'])\n",
    "        conf_mat_dic = conf_mat_dic.reset_index()\n",
    "        #conf_mat_dic.rename(columns={'index':'rank'}, inplace=True)\n",
    "\n",
    "        ### record the best models\n",
    "        conf_mat_dic0.loc[tmp_dic_comp.loc[0, 'index'], df_key] += 1\n",
    "        conf_mat_loo0.loc[tmp_loo_comp.loc[0, 'index'], df_key] += 1\n",
    "        conf_mat_waic0.loc[tmp_waic_comp.loc[0, 'index'], df_key] += 1\n",
    "\n",
    "conf_mat_dic0.to_csv('conf_mat_dic0.csv')\n",
    "conf_mat_loo0.to_csv('conf_mat_loo0.csv')\n",
    "conf_mat_waic0.to_csv('conf_mat_waic0.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
